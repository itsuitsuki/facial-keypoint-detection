{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml')\n",
    "root = tree.getroot()\n",
    "root_dir = 'ibug_300W_large_face_landmark_dataset'\n",
    "\n",
    "bboxes = [] # face bounding box used to crop the image\n",
    "landmarks = [] # the facial keypoints/landmarks for the whole training dataset\n",
    "img_filenames = [] # the image names for the whole dataset\n",
    "\n",
    "for filename in root[2]:\n",
    "\timg_filenames.append(os.path.join(root_dir, filename.attrib['file']))\n",
    "\tbox = filename[0].attrib\n",
    "\t# x, y for the top left corner of the box, w, h for box width and height\n",
    "\tbboxes.append([box['left'], box['top'], box['width'], box['height']]) \n",
    "\n",
    "\tlandmark = []\n",
    "\tfor num in range(68):\n",
    "\t\tx_coordinate = int(filename[0][num].attrib['x'])\n",
    "\t\ty_coordinate = int(filename[0][num].attrib['y'])\n",
    "\t\tlandmark.append([x_coordinate, y_coordinate])\n",
    "\tlandmarks.append(landmark) # relative? \n",
    "\n",
    "landmarks = np.array(landmarks).astype('float32')\n",
    "bboxes = np.array(bboxes).astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IbugTrainingDataset(Dataset):\n",
    "    def __init__(self, img_filenames, bboxes, landmarks, normalize=True, basic_transform=None, albu_transform=None, original=False):\n",
    "        self.img_filenames = img_filenames\n",
    "        self.bboxes = bboxes\n",
    "        self.landmarks = landmarks\n",
    "        self.basic_transform = basic_transform # resize, totensor, normalize\n",
    "        self.albu_transform = albu_transform # albumentations\n",
    "        self.normalize = normalize\n",
    "        self.is_original = original\n",
    "        if not self.normalize:\n",
    "            print('Not normalizing the image')\n",
    "        if not self.basic_transform:\n",
    "            print('No basic transformation')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_filenames[idx]\n",
    "        opened_img = Image.open(img_path).convert('RGB') # range [0, 255] # shape (H, W, C)\n",
    "        bounding_box = self.bboxes[idx]\n",
    "        landmark_ori = self.landmarks[idx] # (68, 2)\n",
    "        x, y, w, h = bounding_box # left, top, width, height\n",
    "        cropped_by_bbox = opened_img.crop((x, y, x+w, y+h))\n",
    "        cropped_by_bbox = np.array(cropped_by_bbox) # range [0, 255]\n",
    "        # to float32\n",
    "        cropped_by_bbox = cropped_by_bbox.astype(np.float32)\n",
    "        if self.normalize:\n",
    "            cropped_by_bbox = cropped_by_bbox / 255.0 - 0.5 # range [-0.5, 0.5]\n",
    "            # print(cropped_by_bbox.dtype)\n",
    "        # adjust the landmark\n",
    "        # landmark2 = landmark - [x, y] # FIXME: broadcast?\n",
    "        landmark = np.zeros_like(landmark_ori)\n",
    "        landmark[:, 0] = landmark_ori[:, 0] - x\n",
    "        landmark[:, 1] = landmark_ori[:, 1] - y\n",
    "        # assert np.all(landmark == landmark2)\n",
    "        # to relative coordinates\n",
    "        if self.albu_transform:\n",
    "            transformed = self.albu_transform(image=cropped_by_bbox, keypoints=landmark)\n",
    "            tfed_im = transformed['image']\n",
    "            landmark = transformed['keypoints']\n",
    "        else:\n",
    "            tfed_im = cropped_by_bbox # (C, H, W)\n",
    "            \n",
    "        landmark = torch.tensor(landmark) # shape: (68, 2)\n",
    "        # relative coordinates\n",
    "        landmark[:, 0] = landmark[:, 0] / w\n",
    "        landmark[:, 1] = landmark[:, 1] / h\n",
    "        # print(tfed_im.shape, w, h)\n",
    "        # tfed_im = torch.tensor(tfed_im)\n",
    "        # print(tfed_im.shape)\n",
    "        if self.basic_transform:\n",
    "            tfed_im = self.basic_transform(tfed_im)\n",
    "        else:\n",
    "            tfed_im = torch.tensor(tfed_im)\n",
    "        if self.is_original:\n",
    "            return tfed_im, landmark, bounding_box, landmark_ori\n",
    "        else:\n",
    "            return tfed_im, landmark\n",
    "    \n",
    "    \n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), # From (C, H, W) to (C, 224, 224)\n",
    "    # grayscale\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Affine(rotate=(-15, 15), translate_percent={'x': 0.1, 'y': 0.1}),\n",
    "    # A.HorizontalFlip(p=0.5),\n",
    "    # A.GaussNoise(p=0.5), # DO WE NEED THIS?\n",
    "    A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    ToTensorV2(),\n",
    "], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "dataset = IbugTrainingDataset(img_filenames, bboxes, landmarks, basic_transform=basic_transform, albu_transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Model Architecture\n",
    "We use ResNet and substitute the first convolutional layer with one that has 1 input channel; we also change the last fully connected layer to output 68*2=136 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, resnet50, resnet34, resnet101, resnet152, resnext50_32x4d, resnext101_32x8d\n",
    "# 32x4d, 32x8d means  32 groups, 4 and 8 are the width of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetforFacialLandmarks(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_landmark=68, dim_landmark=2, resnet=resnet18):\n",
    "        super(ResNetforFacialLandmarks, self).__init__()\n",
    "        self.resnet = resnet(pretrained=True)\n",
    "        # in channel: 1\n",
    "        # fc layer last dim: 68*2\n",
    "        self.resnet.conv1 = nn.Conv2d(in_channels,\n",
    "                                        self.resnet.conv1.out_channels,\n",
    "                                        kernel_size=self.resnet.conv1.kernel_size,\n",
    "                                        stride=self.resnet.conv1.stride, \n",
    "                                        padding=self.resnet.conv1.padding,\n",
    "                                        bias=self.resnet.conv1.bias is not None)\n",
    "        \n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_landmark*dim_landmark)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x).view(x.size(0), -1, 2) # (N, 68, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use ResNeXt50 with 32 groups of convolutions (32 paths) with 4 channels each path. Also, we use a LR of 5e-4 with a batch size of 64. We train for 40 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNetforFacialLandmarks(resnet=resnet50).to(device)\n",
    "# init\n",
    "# def init_weights(m):\n",
    "#     if type(m) == nn.Linear:\n",
    "#         torch.nn.init.xavier_uniform_(m.weight)\n",
    "#         m.bias.data.fill_(0.01)\n",
    "#     elif type(m) == nn.Conv2d:\n",
    "#         torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#         if m.bias is not None:\n",
    "#             m.bias.data.fill_(0.01)\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.SmoothL1Loss() # TODO: Try Huber loss\n",
    "LR = 5e-4\n",
    "# WD = 1e-5\n",
    "# WD = 0.25\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "epochs = 40\n",
    "batch_size = 64\n",
    "\n",
    "# split the dataset\n",
    "seed_everything(42)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# loaders\n",
    "seed_everything(42)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=12)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "seed_everything(42)\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    loop = tqdm(total=len(train_loader), position=0, leave=True)\n",
    "    loop.set_description(f'Epoch {epoch+1}/{epochs}')\n",
    "    for i, (images, landmarks) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        landmarks = landmarks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, landmarks)\n",
    "        # with torch.no_grad():\n",
    "        #     mae = mae_criterion(outputs, landmarks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # loop.set_postfix(train_loss=loss.item(), mae=mae.item())\n",
    "        loop.set_postfix(train_loss=loss.item())\n",
    "        loop.update(1)\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    loop = tqdm(total=len(val_loader), position=0, leave=True)\n",
    "    loop.set_description(f'Epoch {epoch+1}/{epochs}')\n",
    "    with torch.no_grad():\n",
    "        for i, (images, landmarks) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "            landmarks = landmarks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, landmarks)\n",
    "            val_loss += loss.item()\n",
    "            loop.set_postfix(val_loss=loss.item())\n",
    "            loop.update(1)\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "    tqdm.write(f'Epoch {epoch+1}/{epochs}, avg_train_loss: {train_loss}, val_loss: {val_loss}')\n",
    "    # loop.set_postfix(train_loss=train_loss, val_loss=val_loss)\n",
    "    loop.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, epochs+1), train_losses, label='train loss')\n",
    "plt.plot(range(1, epochs+1), val_losses, label='val loss')\n",
    "plt.legend()\n",
    "os.makedirs('3', exist_ok=True)\n",
    "plt.savefig('3/losses.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation MAE\n",
    "# mae_criterion = nn.L1Loss()\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     val_mae = 0.0\n",
    "#     # want original image and landmark\n",
    "#     val_dataset_original = IbugTrainingDataset(img_filenames, bboxes, landmarks, basic_transform=basic_transform, albu_transform=transform, original=True)\n",
    "#     val_loader_original = torch.utils.data.DataLoader(val_dataset_original, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "#     for i, (images, landmarks, bbox, landmark_ori) in enumerate(val_loader_original):\n",
    "#         images = images.to(device)\n",
    "#         landmarks = landmarks.to(device)\n",
    "#         outputs = model(images)\n",
    "#         x, y, w, h = bbox[0]\n",
    "#         outputs[:, :, 0] = outputs[:, :, 0] * w + x\n",
    "#         outputs[:, :, 1] = outputs[:, :, 1] * h + y\n",
    "#         mae = mae_criterion(outputs, landmark_ori.to(device))\n",
    "#         val_mae += mae.item()\n",
    "#     val_mae /= len(val_loader_original)\n",
    "#     print(f'Validation MAE: {val_mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas\n",
    "import xml.etree.ElementTree as ET \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test_parsed.xml')\n",
    "root = tree.getroot()\n",
    "root_dir = 'ibug_300W_large_face_landmark_dataset'\n",
    "\n",
    "bboxes = [] # face bounding box used to crop the image\n",
    "img_filenames = [] # the image names for the whole dataset\n",
    "\n",
    "for filename in root[2]:\n",
    "\timg_filenames.append(os.path.join(root_dir, filename.attrib['file']))\n",
    "\tbox = filename[0].attrib\n",
    "\t# x, y for the top left corner of the box, w, h for box width and height\n",
    "\tbboxes.append([box['left'], box['top'], box['width'], box['height']]) \n",
    "\n",
    "bboxes = np.array(bboxes).astype('float32') \n",
    "print(bboxes.shape)\n",
    "print(len(img_filenames))\n",
    "print(img_filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now have img_filenames and bboxes\n",
    "# for every i in range(len(img_filenames)), \n",
    "# we crop the image, resize it to (224, 224), and then feed it to the model\n",
    "# the output is the landmarks in shape (68, 2)\n",
    "class IBugTestDataset(Dataset):\n",
    "    def __init__(self, img_filenames, bboxes, normalize=True, basic_transform=None):\n",
    "        self.img_filenames = img_filenames\n",
    "        self.bboxes = bboxes\n",
    "        self.basic_transform = basic_transform # resize, totensor\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_filenames[idx]\n",
    "        # grayscale\n",
    "        opened_img = Image.open(img_path).convert('RGB') # range [0, 255] # shape (H, W, C)\n",
    "        bounding_box = self.bboxes[idx]\n",
    "        x, y, w, h = bounding_box # left, top, width, height\n",
    "        cropped_by_bbox = opened_img.crop((x, y, x+w, y+h))\n",
    "        cropped_by_bbox = np.array(cropped_by_bbox) # range [0, 255]\n",
    "        if self.normalize:\n",
    "            cropped_by_bbox = cropped_by_bbox / 255.0 - 0.5\n",
    "            \n",
    "        if self.basic_transform:\n",
    "            cropped_by_bbox = self.basic_transform(cropped_by_bbox)\n",
    "        else:\n",
    "            cropped_by_bbox = torch.tensor(cropped_by_bbox).permute(2, 0, 1) # (C, H, W)\n",
    "        return cropped_by_bbox, bounding_box\n",
    "    \n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), # From (C, H, W) to (C, 224, 224)\n",
    "    # grayscale\n",
    "    transforms.Grayscale(num_output_channels=1), # From (C, H, W) to (1, H, W)\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_dataset = IBugTestDataset(img_filenames, bboxes, basic_transform=basic_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for (images, bbox) in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        outputs = model(images) # (N, 68, 2)\n",
    "        # for every output in outputs, we should get the absolute coordinates\n",
    "        outputs = outputs.cpu().numpy()\n",
    "        for j, output in enumerate(outputs):\n",
    "            x, y, w, h = bbox[j]\n",
    "            x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "            output[:, 0] = output[:, 0] * w + x\n",
    "            output[:, 1] = output[:, 1] * h + y\n",
    "            # output: (68, 2)\n",
    "            preds.append(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternating_names = [] # should be 136 * test_size(1008) = 137088\n",
    "alternating_values = []\n",
    "counter = 0\n",
    "for i in range(len(preds)):\n",
    "    pred = preds[i] # (68, 2)\n",
    "    \n",
    "    for j in range(68):\n",
    "        # alternating_names.append(f'image_{i+1}_keypoints_{j+1}_x')\n",
    "        alternating_names.append(str(counter))\n",
    "        counter += 1\n",
    "        alternating_names.append(str(counter))\n",
    "        counter += 1\n",
    "        # alternating_names.append(f'image_{i+1}_keypoints_{j+1}_y')\n",
    "        \n",
    "        alternating_values.append(pred[j][0]) # x\n",
    "        alternating_values.append(pred[j][1]) # y\n",
    "        \n",
    "        \n",
    "        \n",
    "df = pd.DataFrame({'Id': alternating_names, 'Predicted': alternating_values})\n",
    "\n",
    "os.makedirs('3', exist_ok=True)\n",
    "df.to_csv('3/submission.csv', index=False)\n",
    "!cd 3 && ls -l && kaggle competitions submit -c cs194-26-fall-2022-project-5 -f submission.csv -m \"ResNet50, 25 epochs, 5e-4 LR, 64 bs, 0 WD, Huber, 0.1 color jitter, 0.1 translate, 0.1 rotate\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize 4 test images and their predicted keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 4 images in testset\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes = axes.flatten()\n",
    "for i in range(4):\n",
    "    img = Image.open(img_filenames[i])\n",
    "    ax = axes[i]\n",
    "    ax.imshow(img)\n",
    "    ax.scatter(preds[i][:, 0], preds[i][:, 1], c='r', s=5)\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('3/test_first_4_images.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On images from my collection..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.jpg\n",
    "with torch.no_grad():\n",
    "    im1_ori = Image.open('my_ims/1.jpg')\n",
    "    print(im1_ori.size)\n",
    "    x, y, w, h = 105, 30, 100, 100\n",
    "    im1 = im1_ori.crop((x, y, x+w, y+h)).resize((224, 224))\n",
    "    im1 = im1.convert('L')\n",
    "    im1 = np.array(im1)\n",
    "    im1 = im1 / 255.0 - 0.5 # normalize\n",
    "    im1 = torch.tensor(im1).unsqueeze(0).unsqueeze(0).to(device) # (1, 1, 224, 224)\n",
    "    im1 = im1.float()\n",
    "    output = model(im1) # (1, 68, 2)\n",
    "    output = output.cpu().numpy()\n",
    "    output = output[0] # (68, 2)\n",
    "    rect = plt.Rectangle((x, y), w, h, fill=False, color='blue', linewidth=2)\n",
    "    plt.gca().add_patch(rect)\n",
    "    # absolute coordinates\n",
    "    output[:, 0] = output[:, 0] * w + x\n",
    "    output[:, 1] = output[:, 1] * h + y\n",
    "    # also show the bounding box\n",
    "    \n",
    "    plt.imshow(im1_ori)\n",
    "    plt.scatter(output[:, 0], output[:, 1], c='r', s=5)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('3/my1.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.jpg\n",
    "with torch.no_grad():\n",
    "    im2_ori = Image.open('my_ims/2.jpg')\n",
    "    print(im2_ori.size)\n",
    "    x, y, w, h = 50, 50, 350, 300\n",
    "    im2 = im2_ori.crop((x, y, x+w, y+h)).resize((224, 224))\n",
    "    im2 = im2.convert('L')\n",
    "    im2 = np.array(im2)\n",
    "    im2 = im2 / 255.0 - 0.5 # normalize\n",
    "    im2 = torch.tensor(im2).unsqueeze(0).unsqueeze(0).to(device) # (1, 1, 224, 224)\n",
    "    im2 = im2.float()\n",
    "    output = model(im2) # (1, 68, 2)\n",
    "    output = output.cpu().numpy()\n",
    "    output = output[0] # (68, 2)\n",
    "    rect = plt.Rectangle((x, y), w, h, fill=False, color='blue', linewidth=2)\n",
    "    plt.gca().add_patch(rect)\n",
    "    # absolute coordinates\n",
    "    output[:, 0] = output[:, 0] * w + x\n",
    "    output[:, 1] = output[:, 1] * h + y\n",
    "    # also show the bounding box\n",
    "    \n",
    "    plt.imshow(im2_ori)\n",
    "    plt.scatter(output[:, 0], output[:, 1], c='r', s=5)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('3/my2.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.jpg\n",
    "with torch.no_grad():\n",
    "    im3_ori = Image.open('my_ims/3.jpg')\n",
    "    print(im3_ori.size)\n",
    "    x, y, w, h = 160, 90, 200, 200\n",
    "    im3 = im3_ori.crop((x, y, x+w, y+h)).resize((224, 224))\n",
    "    im3 = im3.convert('L')\n",
    "    im3 = np.array(im3)\n",
    "    im3 = im3 / 255.0 - 0.5 # normalize\n",
    "    im3 = torch.tensor(im3).unsqueeze(0).unsqueeze(0).to(device) # (1, 1, 224, 224)\n",
    "    im3 = im3.float()\n",
    "    output = model(im3) # (1, 68, 2)\n",
    "    output = output.cpu().numpy()\n",
    "    output = output[0] # (68, 2)\n",
    "    rect = plt.Rectangle((x, y), w, h, fill=False, color='blue', linewidth=2)\n",
    "    plt.gca().add_patch(rect)\n",
    "    # absolute coordinates\n",
    "    output[:, 0] = output[:, 0] * w + x\n",
    "    output[:, 1] = output[:, 1] * h + y\n",
    "    # also show the bounding box\n",
    "    \n",
    "    plt.imshow(im3_ori)\n",
    "    plt.scatter(output[:, 0], output[:, 1], c='r', s=5)\n",
    "    plt.axis('off')\n",
    "    plt.savefig('3/my3.jpg', dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
