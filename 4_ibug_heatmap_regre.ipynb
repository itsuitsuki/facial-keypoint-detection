{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET \n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml')\n",
    "root = tree.getroot()\n",
    "root_dir = 'ibug_300W_large_face_landmark_dataset'\n",
    "\n",
    "bboxes = [] # face bounding box used to crop the image\n",
    "landmarks = [] # the facial keypoints/landmarks for the whole training dataset\n",
    "img_filenames = [] # the image names for the whole dataset\n",
    "\n",
    "for filename in root[2]:\n",
    "\timg_filenames.append(os.path.join(root_dir, filename.attrib['file']))\n",
    "\tbox = filename[0].attrib\n",
    "\t# x, y for the top left corner of the box, w, h for box width and height\n",
    "\tbboxes.append([box['left'], box['top'], box['width'], box['height']]) \n",
    "\n",
    "\tlandmark = []\n",
    "\tfor num in range(68):\n",
    "\t\tx_coordinate = int(filename[0][num].attrib['x'])\n",
    "\t\ty_coordinate = int(filename[0][num].attrib['y'])\n",
    "\t\tlandmark.append([x_coordinate, y_coordinate])\n",
    "\tlandmarks.append(landmark) # relative? \n",
    "\n",
    "landmarks = np.array(landmarks).astype('float32')\n",
    "bboxes = np.array(bboxes).astype('float32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IbugTrainingHeatmapDataset(Dataset):\n",
    "    def __init__(self, img_filenames, bboxes, landmarks, normalize=True, basic_transform=None, albu_transform=None, sigma=1):\n",
    "        self.img_filenames = img_filenames\n",
    "        self.bboxes = bboxes\n",
    "        self.landmarks = landmarks\n",
    "        self.basic_transform = basic_transform # resize, totensor, normalize\n",
    "        self.albu_transform = albu_transform # albumentations\n",
    "        self.normalize = normalize\n",
    "        self.sigma = sigma\n",
    "        if not self.normalize:\n",
    "            print('Not normalizing the image')\n",
    "        if not self.basic_transform:\n",
    "            print('No basic transformation')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_filenames[idx]\n",
    "        opened_img = Image.open(img_path).convert('L') # range [0, 255] # shape (H, W)\n",
    "        bounding_box = self.bboxes[idx]\n",
    "        landmark_ori = self.landmarks[idx] # (68, 2)\n",
    "        x, y, w, h = bounding_box # left, top, width, height\n",
    "        cropped_by_bbox = opened_img.crop((x, y, x+w, y+h)) # shape (h, w)\n",
    "\n",
    "        cropped_by_bbox = np.array(cropped_by_bbox) # range [0, 255] # shape (H, W)\n",
    "        cropped_by_bbox = np.expand_dims(cropped_by_bbox, axis=2) # shape (H, W, 1)\n",
    "        # to float32\n",
    "        cropped_by_bbox = cropped_by_bbox.astype(np.float32) # shape (H, W, C)\n",
    "        if self.normalize:\n",
    "            cropped_by_bbox = cropped_by_bbox / 255.0 - 0.5 # range [-0.5, 0.5]\n",
    "            # print(cropped_by_bbox.dtype)\n",
    "        # adjust the landmark\n",
    "        # landmark2 = landmark - [x, y] # FIXME: broadcast?\n",
    "        landmark = np.zeros_like(landmark_ori)\n",
    "        landmark[:, 0] = landmark_ori[:, 0] - x\n",
    "        landmark[:, 1] = landmark_ori[:, 1] - y\n",
    "        # assert np.all(landmark == landmark2)\n",
    "        \n",
    "        \n",
    "        # to relative coordinates\n",
    "        if self.albu_transform:\n",
    "            transformed = self.albu_transform(image=cropped_by_bbox, keypoints=landmark)\n",
    "            tfed_im = transformed['image'] # 68 tokens and each token has 224*224 classes\n",
    "        landmark[:, 1] = landmark[:, 1] / h \n",
    "        landmark[:, 0] = landmark[:, 0] / w \n",
    "        # print(tfed_im.shape, w, h)\n",
    "        # tfed_im = torch.tensor(tfed_im)\n",
    "        # print(tfed_im.shape)\n",
    "        if self.basic_transform:\n",
    "            tfed_im = self.basic_transform(tfed_im) # tfed_im: (C, 224, 224)\n",
    "        else:\n",
    "            tfed_im = torch.tensor(tfed_im)\n",
    "        # heatmap should be (68, 224, 224)\n",
    "        heatmap = self._gaussian_heatmap(landmark * 224, 224, 224, 68)\n",
    "        heatmap = torch.tensor(heatmap)\n",
    "        heatmap = heatmap.float()\n",
    "        landmark = torch.tensor(landmark)\n",
    "        return tfed_im, heatmap, landmark # tfed_im: (C=1, 224, 224), heatmap: (68, 224, 224), landmark: (68, 2)\n",
    "    \n",
    "    def _gaussian_heatmap(self, landmark, height, width, channels):\n",
    "        heatmap = np.ones((channels, height, width)).astype(np.float32)\n",
    "        # landmark: (68, 2)\n",
    "        for i, (x, y) in enumerate(landmark):\n",
    "            if x < 0 or y < 0 or x >= width or y >= height:\n",
    "                # all 1s so that loss is 0\n",
    "                continue\n",
    "            x_coords = np.arange(width) # from 0 to width-1\n",
    "            y_coords = np.arange(height) # 68 tokens and each token has 224*224 classes\n",
    "            x_coords, y_coords = np.meshgrid(x_coords, y_coords)\n",
    "            heatmap[i] = np.exp(-((x_coords - x) ** 2 + (y_coords - y) ** 2) / (2 * self.sigma ** 2))\n",
    "            # optional: normalize the heatmap by max or sum\n",
    "            # heatmap[i] = heatmap[i] / np.sum(heatmap[i])\n",
    "        return heatmap\n",
    "    \n",
    "    \n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), # From (C, H, W) to (C, 224, 224)\n",
    "    # grayscale\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Affine(rotate=(-15, 15), translate_percent={'x': 0.1, 'y': 0.1}),\n",
    "    # A.GaussNoise(p=0.5), # DO WE NEED THIS?\n",
    "    A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    ToTensorV2(),\n",
    "], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "\n",
    "dataset = IbugTrainingHeatmapDataset(img_filenames, bboxes, landmarks, basic_transform=basic_transform, albu_transform=transform, normalize=True, sigma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Model Architecture\n",
    "We use a UNet predicting the probability density heatmap (shape [68, 224, 224]) of the face landmarks, and then find the expected position of the landmarks w.r.t. the heatmap,\n",
    "i.e. a pixelwise classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# load unet\n",
    "from unet import PixelwiseClassificationUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = PixelwiseClassificationUNet(1, 68, 64).to(device)\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Conv2d:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "LR = 1e-3\n",
    "BS = 16\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "num_epochs = 30\n",
    "criterion = nn.MSELoss()\n",
    "mae = nn.L1Loss()\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BS, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BS, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# train\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "    loop.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    for i, (images, heatmaps, landmark) in loop:\n",
    "        # bad heatmap -> jump to next iteration\n",
    "        # i.e. < 1e-4\n",
    "        # if torch.sum(heatmaps) < 1e-4:\n",
    "        #     continue\n",
    "        images = images.to(device) # shape: (B, C, H, W)\n",
    "        landmark = landmark.to(device) # shape: (B, 68, 2)\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_landmark = model(images) # shape: (B, 68, 2)\n",
    "        loss = criterion(predicted_landmark, landmark)\n",
    "        # learn the heatmap: not what we are doing now\n",
    "        # pred_heatmap = model(images, return_heatmap=True) # shape: (B, 68, H*W)\n",
    "        # do BCE\n",
    "        # heatmaps = heatmaps.view(-1, 68, 224*224) # shape: (B, 68, H*W)\n",
    "        # loss = bce(pred_heatmap, heatmaps)\n",
    "        with torch.no_grad():\n",
    "            mae_loss = mae(predicted_landmark, landmark) * 224\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        loop.set_postfix(train_loss=loss.item(), mae=mae_loss.item())\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    loop = tqdm(enumerate(val_loader), total=len(val_loader), leave=True)\n",
    "    loop.set_description(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    with torch.no_grad():\n",
    "        for i, (images, heatmaps, landmark) in loop:\n",
    "            images = images.to(device) # shape: (B, C, H, W)\n",
    "            heatmaps = heatmaps.to(device) # shape: (B, 68, H, W)\n",
    "            landmark = landmark.to(device) # shape: (B, 68, 2)\n",
    "            # Forward pass\n",
    "            predicted_landmark = model(images)\n",
    "            loss = criterion(predicted_landmark, landmark)\n",
    "            mae_loss = mae(predicted_landmark, landmark) * 224\n",
    "            val_loss += loss.item()\n",
    "            loop.set_postfix(val_loss=loss.item(), mae=mae_loss.item())\n",
    "            \n",
    "    val_loss /= len(val_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# %pip install pandas\n",
    "import xml.etree.ElementTree as ET \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "tree = ET.parse('ibug_300W_large_face_landmark_dataset/labels_ibug_300W_test_parsed.xml')\n",
    "root = tree.getroot()\n",
    "root_dir = 'ibug_300W_large_face_landmark_dataset'\n",
    "\n",
    "bboxes = [] # face bounding box used to crop the image\n",
    "img_filenames = [] # the image names for the whole dataset\n",
    "\n",
    "for filename in root[2]:\n",
    "\timg_filenames.append(os.path.join(root_dir, filename.attrib['file']))\n",
    "\tbox = filename[0].attrib\n",
    "\t# x, y for the top left corner of the box, w, h for box width and height\n",
    "\tbboxes.append([box['left'], box['top'], box['width'], box['height']]) \n",
    "\n",
    "bboxes = np.array(bboxes).astype('float32') \n",
    "print(bboxes.shape)\n",
    "print(len(img_filenames))\n",
    "print(img_filenames[0])\n",
    "\n",
    "# we now have img_filenames and bboxes\n",
    "# for every i in range(len(img_filenames)), \n",
    "# we crop the image, resize it to (224, 224), and then feed it to the model\n",
    "# the output is the landmarks in shape (68, 2)\n",
    "class IBugTestDataset(Dataset):\n",
    "    def __init__(self, img_filenames, bboxes, normalize=True, basic_transform=None):\n",
    "        self.img_filenames = img_filenames\n",
    "        self.bboxes = bboxes\n",
    "        self.basic_transform = basic_transform # resize, totensor\n",
    "        self.normalize = normalize\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_filenames[idx]\n",
    "        # grayscale\n",
    "        opened_img = Image.open(img_path).convert('RGB') # range [0, 255] # shape (H, W, C)\n",
    "        bounding_box = self.bboxes[idx]\n",
    "        x, y, w, h = bounding_box # left, top, width, height\n",
    "        cropped_by_bbox = opened_img.crop((x, y, x+w, y+h))\n",
    "        cropped_by_bbox = np.array(cropped_by_bbox) # range [0, 255]\n",
    "        if self.normalize:\n",
    "            cropped_by_bbox = cropped_by_bbox / 255.0 - 0.5\n",
    "            \n",
    "        if self.basic_transform:\n",
    "            cropped_by_bbox = self.basic_transform(cropped_by_bbox)\n",
    "        else:\n",
    "            cropped_by_bbox = torch.tensor(cropped_by_bbox).permute(2, 0, 1) # (C, H, W)\n",
    "        return cropped_by_bbox, bounding_box\n",
    "    \n",
    "basic_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), # From (C, H, W) to (C, 224, 224)\n",
    "    # grayscale\n",
    "    transforms.Grayscale(num_output_channels=1), # From (C, H, W) to (1, H, W)\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_dataset = IBugTestDataset(img_filenames, bboxes, basic_transform=basic_transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BS, shuffle=False)\n",
    "\n",
    "# test\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for (images, bbox) in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        predicted_landmarks = model(images) # [0, 1]\n",
    "        x, y, w, h = bbox[:, 0], bbox[:, 1], bbox[:, 2], bbox[:, 3]\n",
    "        # to devices\n",
    "        x, y, w, h = x.to(device), y.to(device), w.to(device), h.to(device)\n",
    "        # adjust the landmark\n",
    "        predicted_landmarks[:, :, 0] = predicted_landmarks[:, :, 0] * w[:, None] + x[:, None]\n",
    "        predicted_landmarks[:, :, 1] = predicted_landmarks[:, :, 1] * h[:, None] + y[:, None]\n",
    "        preds.append(predicted_landmarks)\n",
    "preds = torch.cat(preds, dim=0) # (N, 68, 2)\n",
    "# preds = preds.cpu().numpy()\n",
    "# preds = preds.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternating_names = [] # should be 136 * test_size(1008) = 137088\n",
    "alternating_values = []\n",
    "counter = 0\n",
    "for i in range(len(preds)):\n",
    "    pred = preds[i] # (68, 2)\n",
    "    # to list\n",
    "    pred = pred.cpu().numpy().tolist()\n",
    "    \n",
    "    for j in range(68):\n",
    "        # alternating_names.append(f'image_{i+1}_keypoints_{j+1}_x')\n",
    "        alternating_names.append(str(counter))\n",
    "        counter += 1\n",
    "        alternating_names.append(str(counter))\n",
    "        counter += 1\n",
    "        # alternating_names.append(f'image_{i+1}_keypoints_{j+1}_y')\n",
    "        \n",
    "        alternating_values.append(pred[j][0]) # x\n",
    "        alternating_values.append(pred[j][1]) # y\n",
    "        \n",
    "        \n",
    "        \n",
    "df = pd.DataFrame({'Id': alternating_names, 'Predicted': alternating_values})\n",
    "\n",
    "os.makedirs('4', exist_ok=True)\n",
    "df.to_csv('4/submission.csv', index=False)\n",
    "!cd 4 && ls -l && kaggle competitions submit -c cs194-26-fall-2022-project-5 -f submission.csv -m \"UNet, 10 epochs, 1e-3 LR, 16 BS, 64 hidden channels\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "180",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
